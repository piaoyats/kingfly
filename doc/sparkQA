1 什么时候会触发TaskSchedulerImpl的resourceOffers？
  有任务提交，有executor注册，有资源更新，有任务失败，（但从打印信息来看每隔指定时间间隔就会，有一个定时机制在这里？）
  
2 如何debug spark
  bin/spark-submit --class org.apache.spark.examples.SparkPi --master local --driver-java-options '-Xdebug -Xrunjdwp:transport=dt_socket,server=y,address=8765' examples/target/scala-2.10/spark-examples-1.1.0-SNAPSHOT-hadoop1.0.4.jar 50 

3 在for循环中对RDD进行操作，那么RDD的操作中会用到的参数需要在for循环里面声明。
如：
val zone: Zone = new Zone(…);
 val arrive: Boolean = …
for (…)
{
     …
     val timesInOneDay: Int = rddDataInTimeSegment.map(line => imsiMapper(line))
                .groupByKey()
                .map(records => getTimesOfOneUser(records, zone, arrive))
                .reduce(_ + _);
       …
 };
这样使用会在红色的代码处抛出异常：
java.lang.IllegalArgumentException: argument type mismatch
注意：单机情况下不会出现这种情况，集群模式才会，原因待分析。
将代码改为：
for (…)
{
     …
val zone: Zone = new Zone(…);
 val arrive: Boolean = …

     val timesInOneDay: Int = rddDataInTimeSegment.map(line => imsiMapper(line))
                .groupByKey()
                .map(records => getTimesOfOneUser(records, zone, arrive))
                .reduce(_ + _);
       …
 };
程序在单机和集群模式下均能正常运行。

中间还尝试过将zone和arrive这两个变量设为全局变量，红色代码改为
.map(records => getTimesOfOneUser(records))
这样在单机模式下也没有问题，但是在集群模式下在getTimesOfOneUser函数中使用zone的代码处会抛出空指针异常。zone的声明如下：
private var zone: Zone = _;
在main函数中使用zone之前会对其赋值，但是赋值不起作用。原因待分析。

4 spark-local-20130923113506-9bc3/15/shuffle_0_123_98这些文件是溢出文件还是其他什么东东，这些数字又是什么含义？
数据在内存中无法存放（MemoryStore）时，会写入本地磁盘（DiskStore）。Shuffle数据
必定会写入磁盘。
数字含义：第一个数字是该shuffle RDD的ID，第二个是父RDD的partition序号，第三个
是reduceID，对应于该shuffle RDD的partition的序号。

5 Connection manager的作用
  ConnectionManager 在启动时会启动一个selectorThread线程，该线程主要负责通过NIO接收或发送数据，接收和发送分别对应sendingConnection和ReadingConnection。

6 shuffled rdd fetch过程是怎样的
6.1 通过shufflemanager 获取reader，（shuffle handler）
6.2 具体fetch时，通过BlockStoreShuffleFetcher去fetch
6.3 而BlockStoreShuffleFetcher上实际是通过ShuffleBlockFetcherIterator来获取数据的，这个获取回来进行一个解包得到最后的iter.
    BlockStoreShuffleFetcher 根据shufflid，reduceid（shuffled rdd要计算的partition的index）来确定去哪些节点获取哪些block数据.
    具体是怎么确定去哪个节点拿哪些block是通过mapOutputTracker来实现的。 SparkEnv.get.mapOutputTracker.getServerStatuses(shuffleId, reduceId)
6.4 ShuffleBlockFetcherIterator是一个(BlockID, values)的迭代器，获取数据需要如下信息：
  1） blockmanager --- 本机的blockmanager, 用来读本地的block
  2） blocksByAddress: Seq[(BlockManagerId, Seq[(BlockId, Long)])] ---- 去哪些节点拿哪些blocks
  3） blockTransferService 或者 shuffleClient --- fetch 远端数据
  4） 序列化器 --- 反序列化数据
  5） 一个重要的配置项spark.reducer.maxMbInFlight --- 同时fetch的最大数据大小,和每次fetch request的大小，最多允许同时5个去远端fetch
  
  官方的解释
  spark.reducer.maxMbInFlight	48	
  Maximum size (in megabytes) of map outputs to fetch simultaneously from each reduce task. 
  Since each output requires us to create a buffer to receive it, this represents a fixed 
  memory overhead per reduce task, so keep it small unless you have a large amount of memory.
  
  在ShuffleBlockFetcherIterator中
  1） hasNext很好确定只要已经处理的blocks数目没有达到需要fetch的block数目就是true
  2） 内部维护一个fetch result队列，这个队列在 发送远程/本地 fetch请求且等待fetch成功后 将数据流添加到 到这个队列
  3） 内部维护一个bytesInFlight变量，每次发送一个fetch请求时，就将本次请求要fetch的数据大小添加到该值，等着吃fetch的数据得到并被消费后再减掉，也就是说最后这个值一定为0，否则fetch失败
  4） next中有一个根据maxMbInFlight来判断是否发送fetch请求的逻辑：只有在fetchRequests不为空且即将fetch的数据加上bytesInFlight不超过maxMbInFlight才会发送fetch请求

7 ConnectionManager initial 和 accept 的连接数为什么是一样多？

8 fetchrequest 是如何切的？
  根据spark.reducer.maxMbInFlight的配置值，比如配置50m，则除以5得到10M，则按10M来切fetch request。如有10个block，1：2，2：5，3：4...这将123的请求包装为一个request
  这里的block是和什么对应？

9 executor 启动是怎么获得driver端的配置的？
  通过akka来实现的，起一个专门获得配置的actor，获得后停掉，比较巧妙：
  // Bootstrap to fetch the driver's Spark properties.
  val executorConf = new SparkConf
  val port = executorConf.getInt("spark.executor.port", 0)
  val (fetcher, _) = AkkaUtils.createActorSystem(
   "driverPropsFetcher", hostname, port, executorConf, new SecurityManager(executorConf))
  val driver = fetcher.actorSelection(driverUrl)
  val timeout = AkkaUtils.askTimeout(executorConf)
  val fut = Patterns.ask(driver, RetrieveSparkProps, timeout)
  val props = Await.result(fut, timeout).asInstanceOf[Seq[(String, String)]] ++
          Seq[(String, String)](("spark.app.id", appId))
  fetcher.shutdown()

10 blockId 和真实的block数据是怎么对应上的？为什么shuffle fetch的时候是通过 ShuffleBlockId(shuffleId, s._1, reduceId) 组装出来的？

